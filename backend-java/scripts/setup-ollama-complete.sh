#!/bin/bash

# Complete Ollama Setup Script for MommyShops
# This script sets up Ollama with all required models

set -e

echo "ü§ñ Setting up Ollama with AI models for MommyShops..."

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama is not installed. Installing Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "‚úÖ Ollama installed successfully"
else
    echo "‚úÖ Ollama is already installed"
fi

# Start Ollama service
echo "üîÑ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to start
echo "‚è≥ Waiting for Ollama to start..."
sleep 10

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚ùå Failed to start Ollama service"
    exit 1
fi

echo "‚úÖ Ollama service is running"

# Pull required models
echo "üì• Pulling required AI models..."

echo "  - Pulling llama3.1 (text generation)..."
ollama pull llama3.1

echo "  - Pulling llava (vision model for image analysis)..."
ollama pull llava

echo "  - Pulling llama3.1:8b (smaller, faster model)..."
ollama pull llama3.1:8b

echo "‚úÖ All models pulled successfully"

# Set environment variables
echo "üîß Setting up environment variables..."

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    touch .env
fi

# Add Ollama configuration to .env
if ! grep -q "OLLAMA_BASE_URL" .env; then
    echo "" >> .env
    echo "# Ollama Configuration" >> .env
    echo "export OLLAMA_BASE_URL=http://localhost:11434" >> .env
    echo "export OLLAMA_MODEL=llama3.1" >> .env
    echo "export OLLAMA_VISION_MODEL=llava" >> .env
fi

# Source the .env file
source .env

echo "‚úÖ Environment variables configured"

# Test the setup
echo "üß™ Testing Ollama setup..."

# Test basic connectivity
if curl -s http://localhost:11434/api/tags | grep -q "llama3.1"; then
    echo "‚úÖ Ollama connectivity test passed"
else
    echo "‚ùå Ollama connectivity test failed"
    exit 1
fi

# Test text generation
echo "  - Testing text generation..."
TEXT_RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1",
    "prompt": "List 3 cosmetic ingredients",
    "stream": false
  }' | jq -r '.response' 2>/dev/null || echo "test")

if [ "$TEXT_RESPONSE" != "null" ] && [ -n "$TEXT_RESPONSE" ]; then
    echo "‚úÖ Text generation working"
else
    echo "‚ö†Ô∏è  Text generation test inconclusive (jq may not be installed)"
fi

echo ""
echo "üéâ Ollama setup completed successfully!"
echo ""
echo "üìã Configuration:"
echo "   - Ollama Base URL: $OLLAMA_BASE_URL"
echo "   - Text Model: $OLLAMA_MODEL"
echo "   - Vision Model: $OLLAMA_VISION_MODEL"
echo ""
echo "üîß Next steps:"
echo "1. Run: ./scripts/start-application.sh"
echo "2. Test with: ./scripts/test-complete-flow.sh"
echo ""
echo "‚ö†Ô∏è  Note: Keep this terminal open to keep Ollama running, or start it manually with 'ollama serve'"